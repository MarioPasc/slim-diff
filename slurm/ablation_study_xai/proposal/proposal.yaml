# =============================================================================
# JS-DDPM XAI-Informed Proposal: Full Model (all improvements)
# =============================================================================
#
# DIAGNOSTIC ANALYSIS (2026-01-25):
#   Baseline: x0_lp_1.5 (severity=0.185, AUC=0.996)
#   Primary artifact: 21% HF energy deficit (wavelet HH L1 = 0.787, target > 0.85)
#   Secondary: Texture smoothness (GLCM dissimilarity d=0.47, homogeneity d=-0.30)
#   Channel attribution: Image channel dominates (56% gradient, 10x ablation impact)
#   Spectral concordance: -0.70 (classifier uses LF features correlated with HF deficit)
#
# XAI ABLATION STUDY:
#   Proposal: Baseline + FFL(alpha=1.35) + self_cond 0.8 + eta 0.1  <--- THIS CONFIG
#   Ablation A: Proposal - FFL (tests HF correction hypothesis)
#   Ablation B: Proposal - self_cond 0.8 (tests LF texture via self-conditioning)
#   Ablation C: Proposal - eta 0.1 (tests LF texture via deterministic sampling)
#
# QUANTITATIVE EVIDENCE (from fresh diagnostics):
#   1. Wavelet HH L1 ratio = 0.787 (21% deficit at finest scale) - PRIMARY TARGET
#   2. Spectral band 4 (0.21-0.50 cycles/px) power ratio = 0.86 (14% deficit)
#   3. Spectral attribution concordance = -0.70: classifier uses LF texture features
#   4. GLCM dissimilarity d=0.47, homogeneity d=-0.30: over-smoothing signature
#   5. Zero FP rate: no synthetic image fools the classifier
#   6. Fisher separability: 121/128 features significant, silhouette=0.448
#
# CHANGES FROM x0_lp_1.5:
#   [C1] loss.mode: mse_lp_norm -> mse_lp_norm_ffl_groups (HF correction)
#   [C2] ffl: enabled with alpha=1.35, patch_factor=1, log_matrix=true
#        alpha=1.35 for focal HF emphasis; patch_factor=1 for full freq resolution
#   [C3] group_uncertainty_weighting: enabled with initial_log_vars [0.0, 1.0]
#        (moderate FFL start: exp(-1.0)=0.37 precision, stronger than 0.22)
#   [C4] self_conditioning.probability: 0.5 -> 0.8 (LF texture refinement)
#   [C5] sampler.eta: 0.2 -> 0.1 (reduced stochasticity, preserves LF structure)
#   [C6] lesion_weight: 1.0 -> 3.0 (lesion Wasserstein 3x higher than overall)
#   [C7] devices: 1 -> 2 with batch=16 each (global batch=32 for stable FFT grads)
#   [C8] max_epochs: 500 -> 1000 (FFL + high self-cond need longer convergence)
#   [C9] patience: 25 -> 100 (dual-loss converges more slowly)
#   [C10] dropout: 0.0 -> 0.05 (regularization for combined loss)
#   [C11] ema.decay: 0.999 -> 0.9995 (smoother averaging with longer training)
#   [C12] texture_quality callback: LBP code 8 + wavelet HH tracking every 25 epochs


# =============================================================================
# Experiment Metadata
# =============================================================================
experiment:
  name: "xai_proposal_full"
  output_dir: "./outputs/xai_proposal_full"
  seed: 33

# =============================================================================
# Data Configuration
# =============================================================================
data:
  cache_dir: "/media/mpascual/Sandisk2TB/research/jsddpm/data/epilepsy/slice_cache"

  # [C5] Increased batch size for stable frequency-domain gradients.
  # FFL computes per-sample FFT — larger batches average out individual
  # spectrum noise, providing smoother gradient updates.
  batch_size: 16
  num_workers: 0  # Keep 0: known CUDA multiprocessing issues
  pin_memory: true

  transforms:
    target_spacing: [1.25, 1.25, 1.25]
    roi_size: [160, 160, 160]
    intensity_norm:
      type: "percentile"
      lower: 0.5
      upper: 99.5
      b_min: -1.0
      b_max: 1.0
      clip: true

  slice_sampling:
    z_range: [34, 115]
    filter_empty_brain: true
    brain_threshold: -0.9
    brain_min_fraction: 0.05

  lesion_oversampling:
    enabled: true
    mode: "balance"
    weight: 3.0

# =============================================================================
# Conditioning Configuration
# =============================================================================
conditioning:
  z_bins: 30
  use_sinusoidal: true
  max_z: 159

  cfg:
    enabled: false
    null_token: 60
    dropout_prob: 0.1

# =============================================================================
# Model Configuration — UNCHANGED from x0_lp_1.5
# =============================================================================
model:
  type: "DiffusionModelUNet"
  spatial_dims: 2
  in_channels: 2
  out_channels: 2

  # Anatomical conditioning: OFF for this step (tested in step 2)
  anatomical_conditioning: false
  anatomical_conditioning_method: "concat"
  cross_attention_dim: 256

  anatomical_encoder:
    hidden_dims: [32, 64, 128]
    downsample_factor: 8
    positional_encoding: "sinusoidal"
    norm_num_groups: 8

  # Architecture: UNCHANGED (tested in step 3)
  channels: [64, 128, 256, 256]
  attention_levels: [false, false, true, true]
  num_res_blocks: 2
  num_head_channels: 32

  norm_name: "GROUP"
  norm_num_groups: 32
  use_class_embedding: true

  # [C8] Small dropout for regularization with combined loss.
  # With two loss components (spatial + frequency), the model has more
  # degrees of freedom and mild regularization helps generalization.
  dropout: 0.05

  resblock_updown: false
  with_conditioning: false

# =============================================================================
# Scheduler Configuration — UNCHANGED
# =============================================================================
scheduler:
  type: "DDPM"
  num_train_timesteps: 1000
  schedule: "cosine"
  beta_start: 0.0015
  beta_end: 0.0195
  sig_range: 6.0

  # x0 prediction: confirmed best by diagnostic data.
  prediction_type: "sample"

  # KEEP clip_sample=true for x0 prediction.
  # With prediction_type="sample", the model output IS the predicted x0.
  # Clipping to [-1, 1] is a VALID physical constraint (normalized MRI range).
  # Without clipping, predicted values outside [-1, 1] are physically meaningless
  # and would corrupt the FFT computation in FFL.
  clip_sample: true
  clip_sample_range: 1.0

# =============================================================================
# Sampler Configuration
# =============================================================================
sampler:
  type: "DDIM"
  num_inference_steps: 500

  # [C5] Reduced stochasticity (eta 0.2 -> 0.1).
  # Diagnostic evidence: concordance=-0.70 shows the classifier detects LF texture
  # anomalies that correlate with HF deficit. Lower eta reduces stochastic noise
  # injection during DDIM sampling, preserving the learned LF texture structure
  # from the deterministic trajectory.
  #
  # DDIM: x_{t-1} = sqrt(alpha_{t-1}) * x0_pred + sqrt(1-alpha_{t-1}-sigma^2) * eps_pred + sigma * noise
  # eta controls sigma: sigma = eta * sqrt((1-alpha_{t-1})/(1-alpha_t)) * sqrt(1-alpha_t/alpha_{t-1})
  # eta=0.1 gives ~10% of the DDPM noise, preserving sample diversity while reducing
  # texture degradation from stochastic noise.
  eta: 0.1

  guidance_scale: 1.0

# =============================================================================
# Training Configuration
# =============================================================================
training:
  # [C7] Per-GPU batch size 16, with 2 GPUs -> global batch 32
  # Larger effective batch stabilizes FFT-based FFL gradients
  batch_size: 16
  num_workers: 0
  pin_memory: true

  accelerator: "gpu"
  devices: 2
  strategy: "ddp"

  # [C4] Self-conditioning at 0.8 (up from 0.5).
  # Diagnostic evidence: spectral attribution concordance=-0.70 means the classifier
  # detects LF texture anomalies that correlate with the HF deficit, rather than
  # detecting HF deficit directly. High self-conditioning probability (0.8) gives
  # the model iterative access to its own x0 estimate, enabling LF texture
  # refinement across denoising steps.
  #
  # Mechanistically: at timestep t, the model sees its prediction from t+1,
  # allowing it to refine texture consistency rather than regenerating from scratch.
  # This is particularly important for subtle texture details (GLCM dissimilarity
  # d=0.47, homogeneity d=-0.30).
  #
  # Reference: Chen et al. "Analog Bits" (2022)
  self_conditioning:
    enabled: true
    probability: 0.8

  optimizer:
    type: "AdamW"
    lr: 1.0e-4
    weight_decay: 1.0e-4
    betas: [0.9, 0.999]
    eps: 1.0e-8

  lr_scheduler:
    type: "CosineAnnealingLR"
    T_max: null
    eta_min: 1.0e-6

  # [C6] Longer training for FFL convergence.
  # FFL introduces a secondary optimization landscape in frequency domain.
  # The model must first learn spatial structure (Lp norm), then refine
  # spectral content (FFL). This sequential convergence needs more epochs.
  max_epochs: 1000
  max_steps: null

  precision: "16-mixed"
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"
  accumulate_grad_batches: 1

  val_check_interval: 1.0
  check_val_every_n_epoch: 1

  # [C7] More patience for dual-loss convergence.
  early_stopping:
    enabled: true
    monitor: "val/loss"
    patience: 100
    mode: "min"

  # [C11] EMA decay 0.999 -> 0.9995 (smoother averaging with longer training).
  ema:
    enabled: true
    decay: 0.9995
    update_every: 1
    update_start_step: 0
    store_on_cpu: true
    use_buffers: true
    use_for_validation: true
    export_to_checkpoint: true

# =============================================================================
# Loss Configuration — THE KEY CHANGE
# =============================================================================
loss:
  # [C1] Combined Lp norm + FFL with group uncertainty weighting.
  # This is the ONLY structural change from x0_lp_1.5 in this ablation step.
  mode: "mse_lp_norm_ffl_groups"

  # Lp norm p=1.5: UNCHANGED (confirmed best by data)
  # Gradient ∝ |x|^{p-1}, sub-linear growth preserves sharp features
  lp_norm:
    p: 1.5

  uncertainty_weighting:
    enabled: false
    initial_log_vars: [0.0, 0.0]
    learnable: true
    clamp_range: [-5.0, 5.0]

  # [C3] Kendall group uncertainty weighting.
  # Learns optimal balance between:
  #   Group 0: Lp norm image + Lp norm mask (spatial quality)
  #   Group 1: FFL (spectral quality)
  #
  # Initial log_vars [0.0, 1.0] (moderate FFL start):
  #   Group 0 precision: exp(-0) = 1.0 (full weight)
  #   Group 1 precision: exp(-1.0) = 0.37 (moderate initial weight)
  #
  # Justification for [0.0, 1.0] vs [0.0, 1.5]:
  #   The diagnostic analysis shows the HF deficit (wavelet HH L1 = 0.787) is
  #   the PRIMARY artifact causing near-perfect classification (AUC=0.996).
  #   A 21% energy deficit requires meaningful FFL contribution from early training.
  #   exp(-1.0) = 0.37 gives FFL 37% effective weight, vs only 22% with exp(-1.5).
  #   The learnable log_vars will adjust this balance during training.
  #
  # Reference: Kendall et al. "Multi-Task Learning Using Uncertainty" (CVPR 2018)
  group_uncertainty_weighting:
    enabled: true
    initial_log_vars: [0.0, 1.0]
    learnable: true
    clamp_range: [-5.0, 5.0]
    intra_group_weights: [1.0, 1.0, 1.0]

  # [C2] Focal Frequency Loss.
  #
  # Parameters tuned based on diagnostic analysis (2026-01-25):
  #
  # alpha=1.35: Moderately focal weighting.
  #   Wavelet HH L1 ratio = 0.787 indicates 21% HF deficit at finest scale.
  #   alpha > 1 gives superlinear emphasis to hard-to-synthesize frequencies.
  #   1.35 is aggressive enough to address the deficit without the 39x HF excess
  #   seen in epsilon experiments (which used epsilon prediction, not x0).
  #   Reference: Jiang et al. ICCV 2021 recommend alpha in [1.0, 2.0].
  #
  # patch_factor=1: No patching (full 160x160 FFT).
  #   The HF deficit is at the finest wavelet scale (L1: 40-80 px wavelength).
  #   Full-image FFT provides maximum frequency resolution (80 bins to Nyquist).
  #   Patching would reduce resolution: patch_factor=2 gives only 40 bins,
  #   potentially missing the critical 0.21-0.50 cycles/px band where ratio=0.86.
  #
  # log_matrix=true: Numerical stability.
  #   MRI frequency content spans 4+ orders of magnitude (10^7 at DC to 10^1 at HF).
  #   log(1+x) normalizes the weight matrix to prevent DC dominance.
  ffl:
    enabled: true
    loss_weight: 1.0
    alpha: 1.35
    patch_factor: 1
    ave_spectrum: false
    log_matrix: true
    batch_matrix: false

  # [C6] Strong lesion weighting.
  # Diagnostic evidence: Per-tissue Wasserstein distance analysis shows
  # lesion regions have 3x higher distribution mismatch than overall:
  #   - Lesion Wasserstein: 0.062
  #   - Overall Wasserstein: 0.022
  #   - Ratio: 2.8x
  #
  # 3.0x upweighting ensures the model attends to lesion texture quality,
  # compensating for the small pixel count of lesion regions (~0.8% of image).
  # Without upweighting, lesion errors contribute minimally to the gradient.
  lesion_weighted_image:
    enabled: true
    lesion_weight: 3.0
    background_weight: 1.0

  lesion_weighted_mask:
    enabled: true
    lesion_weight: 3.0
    background_weight: 1.0

# =============================================================================
# Postprocessing Configuration — UNCHANGED
# =============================================================================
postprocessing:
  zbin_priors:
    enabled: false
    priors_filename: "zbin_priors_brain_roi.npz"
    prob_threshold: 0.20
    dilate_radius_px: 1
    gaussian_sigma_px: 0.7
    min_component_px: 500
    n_first_bins: 5
    max_components_for_first_bins: 3
    relaxed_threshold_factor: 0.1
    fallback: "prior"
    apply_to: ["visualization"]
    background_value: -1.0
    use_prior_directly: true

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  log_every_n_steps: 50

  logger:
    type: "wandb"
    wandb:
      project: "jsddpm-epilepsy"
      entity: "mario-pg02-icai-org"
      name: "xai_proposal_full"
      tags: ["epilepsy", "diffusion", "jsddpm", "xai_ablation", "x0", "ffl_alpha1.35", "lp_1.5", "self_cond_0.8", "eta_0.1"]
      notes: "XAI-informed proposal: x0_lp_1.5 + FFL(alpha=1.35,pf=1) + self_cond 0.8 + eta 0.1. Target: wavelet HH L1 0.787->0.85+"
      offline: true

  checkpointing:
    save_top_k: 1
    monitor: "val/loss"
    mode: "min"
    save_last: false
    every_n_epochs: 1
    filename: "jsddpm-{epoch:04d}-{val_loss:.4f}"

  callbacks:
    gradient_norm:
      enabled: true
      log_every_n_steps: 100

    snr:
      enabled: true
      log_every_n_epochs: 5
      n_samples: 100

    prediction_quality:
      enabled: true
      log_every_n_epochs: 5
      n_samples: 100
      timestep_bins: 10

    # [C12] Texture quality tracking: wavelet HH + GLCM dissimilarity + LBP
    # Monitors the artifact markers identified by XAI analysis (2026-01-25):
    #
    # Key metrics and targets:
    #   1. wavelet_hh_ratio: synth/real HH L1 energy (target: >0.85, baseline: 0.787)
    #      Addresses 21% HF deficit - PRIMARY ARTIFACT
    #   2. glcm_dissimilarity_ratio: synth/real (target: ~1.0, baseline d=0.47)
    #      Addresses over-smoothing - texture variation deficit
    #   3. lbp_code8_ratio: synth/real (target: ~1.0)
    #      Addresses LF texture anomaly
    #
    # Only runs on rank 0 in DDP to avoid duplicate computation.
    # Uses fast generation (50 steps vs 500) for reasonable callback runtime (~2-3 min).
    texture_quality:
      enabled: true
      log_every_n_epochs: 25
      n_samples: 32               # Small sample for fast callback
      lbp_radius: 1
      lbp_n_points: 8
      wavelet: "db4"
      compute_full_texture: false
      run_on_first_epoch: true
      fast_inference_steps: 50    # Fast DDIM (vs 500 for full quality)
      batch_size: 8               # Batched generation

# =============================================================================
# Visualization Configuration
# =============================================================================
visualization:
  enabled: true
  every_n_epochs: 1

  z_bins_to_show: null
  n_samples_per_condition: 1

  overlay:
    enabled: true
    alpha: 0.5
    color: [255, 0, 0]
    threshold: 0.3

  save_png: true
  save_npz: false

# =============================================================================
# Generation Configuration
# =============================================================================
generation:
  n_per_condition: 100
  z_bins: null
  classes: [0, 1]
  output_format: "npz"
  save_individual: true
  create_index_csv: true
