# Epilepsy Lesion Segmentation Module

K-fold cross-validation experiments for epilepsy lesion segmentation using multiple MONAI models.

## Features

- **4 MONAI Models**: UNet, DynUNet, UNet++, SwinUNETR
- **Subject-level K-fold CV**: Prevents data leakage between folds
- **Real + Synthetic Data**: Configurable mixing ratios
- **Dual Logging**: Weights & Biases + CSV
- **Class Balancing**: Weighted sampling for imbalanced datasets
- **Comprehensive Metrics**: Dice Score, Hausdorff Distance 95

## Quick Start

### Training UNet with Real Data Only (Baseline)

```bash
python -m src.segmentation.training.train_kfold --model unet
```

### Training with Synthetic Data

```bash
# 50% synthetic data
python -m src.segmentation.training.train_kfold \
    --model unet \
    --synthetic-ratio 0.5

# 100% synthetic data (1:1 ratio)
python -m src.segmentation.training.train_kfold \
    --model dynunet \
    --synthetic-ratio 1.0
```

### Running Specific Folds

```bash
# Train only folds 0, 1, 2
python -m src.segmentation.training.train_kfold \
    --model unetplusplus \
    --folds 0 1 2
```

### Quick Test Run

```bash
# Single fold, few epochs for testing
python -m src.segmentation.training.train_kfold \
    --model unet \
    --folds 0 \
    --max-epochs 5 \
    --batch-size 8
```

## Configuration

The module uses a hierarchical YAML configuration system:

- **Master Config**: `src/segmentation/config/master.yaml`
- **Model Configs**: `src/segmentation/config/models/{model_name}.yaml`

### Key Configuration Sections

```yaml
experiment:
  name: "segmentation_unet"
  output_dir: "./outputs/segmentation"
  seed: 42

k_fold:
  n_folds: 5
  exclude_test: true  # Exclude test.csv from k-fold
  stratify_by: "has_lesion_subject"

data:
  synthetic:
    enabled: false
    ratio: 0.0  # 0.0 = real only, 1.0 = equal mix

training:
  batch_size: 16
  max_epochs: 100
  class_balancing:
    enabled: true
    lesion_weight: 5.0  # Oversample lesion slices 5x
```

## Directory Structure

```
src/segmentation/
├── config/
│   ├── master.yaml              # Main configuration
│   └── models/
│       ├── unet.yaml
│       ├── dynunet.yaml
│       ├── unetplusplus.yaml
│       └── swinunetr.yaml
├── data/
│   ├── dataset.py               # Dataset with mask conversion {-1,+1}→{0,1}
│   ├── splits.py                # Subject-level k-fold splitter
│   └── transforms.py            # MONAI augmentation
├── models/
│   └── factory.py               # Model instantiation
├── metrics/
│   └── segmentation_metrics.py  # Dice, HD95
├── training/
│   ├── lit_module.py            # PyTorch Lightning module
│   ├── runners.py               # K-fold orchestration
│   └── train_kfold.py           # CLI entrypoint
├── callbacks/
│   └── logging_callbacks.py     # CSV logging
├── utils/
│   ├── seeding.py
│   ├── io.py
│   ├── config.py
│   └── logging.py
└── tests/
    └── test_smoke.py
```

## Output Structure

After running k-fold training:

```
outputs/segmentation/
├── fold_0/
│   ├── checkpoints/
│   │   ├── seg-epoch_0042-val_dice_0.7234.ckpt
│   │   └── last.ckpt
│   ├── csv_logs/
│   │   └── fold_0_metrics.csv
│   ├── logs/                    # WandB logs
│   └── config.yaml              # Fold-specific config
├── fold_1/
├── ...
├── fold_4/
└── kfold_results.json           # Aggregated results
```

### Example kfold_results.json

```json
{
  "mean_dice": 0.7125,
  "std_dice": 0.0234,
  "min_dice": 0.6891,
  "max_dice": 0.7412,
  "fold_results": [
    {"fold": 0, "best_dice": 0.7234, "best_model_path": "..."},
    ...
  ]
}
```

## CLI Reference

```bash
python -m src.segmentation.training.train_kfold [OPTIONS]

Required:
  --model {unet,dynunet,unetplusplus,swinunetr}

Optional:
  --config PATH              Master config path
  --folds INT [INT ...]      Specific folds to run
  --synthetic-ratio FLOAT    Ratio of synthetic to real data
  --output-dir PATH          Override output directory
  --seed INT                 Random seed
  --max-epochs INT           Maximum epochs
  --batch-size INT           Batch size
```

## Models

### UNet (Recommended Baseline)
- Fast training (~5 min/epoch on GPU)
- ~1M parameters
- Good baseline performance

### DynUNet
- Flexible architecture
- ~2-5M parameters
- Better for varied lesion sizes

### UNet++ (BasicUNetPlusPlus)
- Dense skip connections
- ~3-8M parameters
- Best for small lesions

### SwinUNETR
- Transformer-based
- ~20-50M parameters
- Slower but potentially highest performance

## Data Format

### Real Data (Slice Cache)
- Location: `/media/mpascual/Sandisk2TB/research/epilepsy/data/slice_cache`
- Format: NPZ files with `image` (128×128, [-1,1]) and `mask` (128×128, {-1,+1})
- Index: `train.csv`, `val.csv`, `test.csv`

### Synthetic Data
- Generated by `src/diffusion/training/runners/generate.py`
- Same NPZ format as real data
- Index: `generated_samples.csv`

### Mask Conversion
The dataset automatically converts masks from diffusion format {-1, +1} to segmentation format {0, 1}:

```python
# In dataset.__getitem__()
mask_binary = (mask > 0.0).astype(np.float32)  # {-1,+1} → {0,1}
```

## K-Fold Strategy

**Subject-level splitting** ensures no data leakage:
- All slices from a subject stay in the same fold
- Stratified by lesion presence (subjects with/without lesions balanced)
- Test set subjects excluded from k-fold entirely

Example with 116 subjects, 5 folds:
- Fold 0: Train=93 subjects, Val=23 subjects
- Fold 1: Train=93 subjects, Val=23 subjects
- ...
- Test: 25 subjects (excluded from k-fold)

## Metrics

### Dice Score Coefficient (DSC)
- Primary metric for segmentation quality
- Range: [0, 1], higher is better
- Target: >0.7 for good performance

### Hausdorff Distance 95 (HD95)
- Boundary accuracy metric
- Lower is better
- Handles empty masks (returns NaN if no foreground)

## Class Imbalance Handling

Only ~12% of slices contain lesions. Two strategies:

1. **Weighted Sampler** (default, `lesion_weight=5.0`)
   - Oversamples lesion slices during training
   - Sees lesion examples 5x more frequently

2. **Weighted Loss**
   - DiceCE loss already handles class imbalance well

## Augmentation

Spatial transforms (image + mask):
- Random flip (50%)
- Random rotation ±15° (50%)
- Random scale 0.9-1.1 (30%)

Intensity transforms (image only):
- Gaussian noise (20%)
- Gaussian smoothing (20%)
- Gamma adjustment (30%)

## Testing

Run smoke tests:

```bash
# Test configuration loading
pytest src/segmentation/tests/test_smoke.py::TestConfiguration -v

# Test model instantiation
pytest src/segmentation/tests/test_smoke.py::TestModelFactory -v

# Test all
pytest src/segmentation/tests/test_smoke.py -v
```

## Troubleshooting

### Out of Memory (OOM)
```bash
# Reduce batch size
python -m src.segmentation.training.train_kfold \
    --model unet \
    --batch-size 8
```

### Slow Training
```bash
# Use smaller model
python -m src.segmentation.training.train_kfold \
    --model unet  # Fastest

# Reduce workers
# Edit master.yaml: training.num_workers: 2
```

### NaN Loss
- Check data normalization (should be in [-1, 1])
- Reduce learning rate in config
- Enable gradient clipping (already enabled at 1.0)

## Integration with Diffusion Module

1. **Generate synthetic data**:
```bash
python -m src.diffusion.training.runners.generate \
    --config src/diffusion/config/jsddpm.yaml \
    --ckpt /path/to/checkpoint.ckpt \
    --out_dir ./outputs/synthetic
```

2. **Train segmentation with synthetic**:
```bash
python -m src.segmentation.training.train_kfold \
    --model unet \
    --synthetic-ratio 0.5 \
    --config src/segmentation/config/master.yaml
```

Note: Update `data.synthetic.samples_dir` in master.yaml to point to synthetic output directory.

## Best Practices

1. **Always run real-only baseline first** to establish performance floor
2. **Use subject-level k-fold** to prevent optimistic bias
3. **Monitor HD95** in addition to Dice (boundary accuracy matters)
4. **Start with UNet** before trying more complex models
5. **Check fold balance** using printed statistics
6. **Save configs** with results for reproducibility

## Citation

If you use this segmentation module, please cite:

```bibtex
@software{jsddpm_segmentation,
  title = {K-Fold Segmentation Module for JS-DDPM Epilepsy Project},
  author = {Your Name},
  year = {2024},
  url = {https://github.com/your-repo}
}
```

## License

MIT License - See main project LICENSE file.
